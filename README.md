
```
                                            !                 
                                           lll                /\\       /\\     
                                        ///     \             /\ /\\   /\\\   
                    .---.              |||       |            /\\ /\\ / /\\   /\\    /\ /\\\   /\\\   /\\   
        .----.    .'     '.    P_______|[][][][][]_______P    /\\  /\\  /\\ /\\  /\\  /\\    /\\    /\\  /\\ 
      .'      '..'         '-.++++++++=o0@%@%@%o|||++++++++   /\\   /\  /\\/\\   /\\  /\\   /\\    /\\    /\\
    .'          i-._          |l l l ll [] [] []| l l l l l   /\\       /\\/\\   /\\  /\\    /\\    /\\  /\\ 
_.'                          |_______| []=== []| |_______|   /\\       /\\  /\\ /\\\/\\\      /\\\   /\\  
                                      |  || ||  | /           
                           o          |__|| ||__|/                        /\\\\\\\             /\\  
                       ~)-"|()                     o/       o             /\\    /\\           /\\        
                      .(_/^\[]    [|]     (|)   ~) |[]   ~)_/\]    u      /\\    /\\   /\\     /\\   /\\     
            /\           ./|~|\   ~)[|]   ~)[|] .'(_/^\'..'(_/^\. ~)  \]    /\\\\\\\   /\\  /\\  /\\ /\\  /\\
       _v   ,,         /'/'/'|''(_/^\'.'(_/^\'   /|~|\    /|~|\''(_/^\    /\\       /\\    /\\ /\\/\\    /\\
      /`|   &&.                 /|~|\   /|~|\  / / / |  / / / |  /|~|\    /\\        /\\  /\\  /\\ /\\  /\\
      `-\`-&&&&&.              / / / | / / / |                  / / / |   /\\          /\\    /\\\   /\\
         \&&&&&&&\              
          &&#""&& \ 			  
         / |   |\  x		        Multiagent Architecture for Probabilistic Open-ended Learning
         \ |  / /      
```


# Marco Polo

This is a Mobius logic redesign of the ePOET software from Uber/openAI.


# Setup
We provide 2 setup scripts, one to create a `Conda` environment via [mamba](https://mamba.readthedocs.io/en/latest/index.html) and one to create a virtual environment via `venv`. The `Conda` method is up-to-date, and is the recommended method. 

* Conda: [setup_mp_conda.sh](./setup_mp_conda.sh), sets up a `Conda` environment named `poet`
* Venv: [setup_mp_venv.sh](./setup_mp_venv.sh), sets up a `venv` named `poet_venv`


# Run
We provide a run script ([run_MP.sh](./run_MP.sh)) to start a new run or resume an old run from a provided checkpoint. The files will setup appropriate output directories if they do not exist. They must be run from the base directory of this repository. 

* Start new run: `./run_MP.sh /PATH/TO/OUTPUT/DIR /PATH/TO/PARAMETERS.yml`
* Resume run: `./run_MP.sh /PATH/TO/OUTPUT/DIR /PATH/TO/CHECKPOINT/DIRECTORY`

It is also possible to run it within VSCode or using the VSCode debugger, but I (JB) don't know how to do that.


# Design Notes
There is a small flow diagram [here](https://drive.google.com/file/d/1X_7Ii_NPLQK2RELg5ZmKrsIydFVF0PO2/view?usp=share_link) and [here](https://drive.google.com/file/d/1CvbseduPw5WpO0xMkYabS7z1S0UMvDoP/view) showing a rough outline of the current design ethos. The goal is 
significant amounts of [composition](https://en.wikipedia.org/wiki/Composition_over_inheritance?wprov=sfti1) so that things are independent but also reuse code as much as possible. 

Long-term goals involve integration of alternative optimization platforms (such as [SB3](https://stable-baselines3.readthedocs.io/en/master/) and [Ray](https://www.ray.io/)), improved support for novel environments, and easier adaptation of core POET routines.  

## Directory Design
The full directory layout can be generated by running `tree ~/epoet-modius`.
Below is a short summary, with descriptions of what goes in each directory.

These files are organized as follows:
```
. Repository Root Directory
|--   The base directory includes environment setup files, parameter files, run files, and analysis files
|-- run_MP.sh
|--   Bash run script, for getting parameters, setting up directories, and logging to file/console.  
|-- master.py
|--   Main run file. This takes external parameters, either cmdline or yml, and creates the simulation.
|-- marco_polo
|   |--   This is the root of the simulation. All files for use within the sim are here.
|   |-- poet_algo.py
|   |--   This is the secondary run file, with the main simulation loop.
|   |-- algorithms
|   |--   This directory is for algorithmic implementations of the "POET methods". 
|   |--   It has the basic novelty, evoluation, and transfer algorithms.
|   |--   When designing new algorithms (such as using Hodge theory for transfers/reproduction), this is where they go.
|   |-- cores
|   |--   This is where the core managers live.
|   |--   Each core gets its own directory, and has a unique combination of env, optimizer, and thread manager. 
|   |-- |-- ePoet
|   |-- |--   This directory is the classic, ePoet core, from the corresponding paper.
|   |-- |-- <new core manager directory>
|   |-- |--   Placeholder for the next core, probably managing an optimization routine from SB3.
|   |-- envs
|   |--   This is where the environment definition files go.
|   |--   Each environment will have its own directory.
|   |   |-- BipedalWalker
|   |   |--   This directory has the BipedalWalker env from POET, as well as the CPPN class.
|   |   |-- <new env directory>
|   |   |--   This is a placeholder for the next, new env
|   |-- optimizers
|   |--   This directory holds compute managers and the corresponding optimization routines.
|   |--   Each manager gets its own directory, which has the manager file and a "module" directory within.
|   |-- |-- uber_es
|   |-- |--   The uber_es manager uses the fiber multi-threader provided by uber and implements their evolutionary strategy
|   |-- |-- <new optimizer>
|   |-- |--   A placeholder for another compute manager and optimization routine - possibly based on SB3.
|   |-- tools
|   |--   Aux directory for smaller, algorithm-agnostic routines.
|   |--   These routines do not import any of the other modules.
|   |-- 
```

## Required Class Functionality
While no formal interfaces are implemented (but could be, see [here](https://stackoverflow.com/a/51273196) and [here](https://realpython.com/python-interface/)), there are necessary requirements from each class in order to promote interoperability.

* cores: `poet_algo.PopulationManager.evolve_population()` relies on 4 functions from the `Core()` class.
  * Functions:
    1. `iteration_update(int)` - idk
    2. `reproduce(int)` - generate novel enviroments and pair with agents
    3. `optimize(int)` - play agents and update models for improved performance
    4. `transfer(int)` - perform agent transfers between teams or environments
  * Behaviors: are there side effects that I want to document?
* managers: is this true? Do we need this? possibly only because of the poet algorithm?
  * Functions:
    1. `optimize_chunk(list of tuples of (env, agent))` - 
    2. `optimize_step(list of tuples of (env, agent), boolean)` - 
    3. `evaluate(list of tuples of (env, agent), boolean)` - 
  * Behaviors:



## Next Steps

idk, something intelligent.
Currently these involve documentation.

### Create wrappers for
* Gym_to_Gymnasium - Transforms multiagent gym game to multiagent gymnasium game
* Gymnasium_to_Gym - Transforms multiagent gymnasium game to multiagent gym game
* Single_Agent_to_Multi - Adds approprate dictionary structure to single agent games












### Troubleshotting Log Permission error:
In your venv in sitepackages/fiber in _init_.py line 34 there should be `init_fiber(proc_name="Master")` change "master" to your name.

## Interfaces for common objects:

```
######################################
## Model Interface:
######################################
RL Agent model forma single agent. We're defaulting to pytorch for implamentation but in general model will depend on the opmitimization manager being used. 

Model(args:Namespace, seed:Int)

Methods
-------------
checkpoint(folder: Path)
	Save the model in its format in the given folder. File name is specified in this method. Folder name should differentiate between different agents. 
	
reload(folder: Path)
	Load the model from the given folder. File name is specified in this method. Folder name should differentiate between different agents. 
	
get_step(obs: np.array) → np.ndarray
	Perform forward pass through network.


######################################
## Env Interface (Follows Petting Zoo)
######################################

Gym environment, always assumed to be a multiplayer interface based on gymnasium. Single player env's can be wraped to multiplayer and gym can be wrapped to gymnasium and visa versa. 

Attributes
-------------

agents: list[AgentID]
	A list of the names of all current agents, typically integers. May changed as environment progresses 
num_agents: int
	The length of the agents list.
possible_agents: list[AgentID]
	A list of all possible_agents the environment could generate. Equivalent to the list of agents in the observation and action spaces. This cannot be changed through play or resetting.
max_num_agents: int
	The length of the possible_agents list.
observation_spaces: Dict[AgentID, gym.spaces.Space]
	A dict of the observation spaces of every agent, keyed by name. This cannot be changed through play or resetting.
action_spaces: Dict[AgentID, gym.spaces.Space]
	A dict of the action spaces of every agent, keyed by name. This cannot be changed through play or resetting.


Methods
-------------

step(actions: Dict[str, ActionType]) → Tuple[Dict[str, ObsType], Dict[str, float], Dict[str, bool], Dict[str, bool], Dict[str, dict]]
	Receives a dictionary of actions keyed by the agent name.
	Returns the observation dictionary, reward dictionary, terminated dictionary, truncated dictionary and info dictionary, where each dictionary is keyed by the agent.
reset(seed: int | None = None, options: dict | None = None) → Dict[str, ObsType]
	Resets the environment.
seed(seed=None)
	Reseeds the environment (making it deterministic).
render() → None | np.ndarray | str | List
	Displays a rendered frame from the environment, if supported.
close()
	Closes the rendering window.
state() → ndarray
	Returns the state.
observation_space(agent: str) → Space
	Returns the observation space for given agent.
action_space(agent: str) → Space


######################################
## EnvParams Interface
######################################

Holds the parameters needed to instantiate a gym env. Gym env's should be interchangeable, with all relivant information being passed by the EnvParams. 

EnvParams(args:Namespace, seed: int)

Methods
-------
get_mutated_params(self) → EnvParams

checkpoint(folder: Path)
	Save the model in its format in the given folder. File name is specified in this method. Folder name should differentiate between different agents. 
	
reload(folder: Path)
	Load the model from the given folder. File name is specified in this method. Folder name should differentiate between different agents. 


######################################
## Manager Interface
######################################

Compute Manager Class

This class handles all compute by providing a standardized interface for 
optimization and evaluation. This manager is based on the multiprocessing 
asynchronous pool.

Manager(args:Namespace)

Attributes
----------
setup_args : argparse.Namespace
    Seems to be like a dictionary
    This stores all of the simulation parameters
np_random : Generator
    Numpy random generator
crew : Multiprocessing Pool
    Worker pool for asynchronous evaluation

Methods
-------
Manager(argparse.Namespace, List[Func])
    Initializer, stores functions, creates worker pool
checkpoint(folder: Path)
    Store output
reload(folder: Path)
    Reload output from checkpoint
evaluate(tasks: List[(env, team)], epochs: int, verbose: bool)
    Main evaluation function
optimize_step(tasks: List[(env, team)], epochs: int, opt_iter: int)
    Single optimization step
optimize_chunk(tasks: List[(env, team)], epochs: int, verbose: bool)
    Main optimization function, with evals and video generation
```
